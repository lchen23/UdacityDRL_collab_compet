{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.09000000171363354\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.09000000171363354\n",
      "Score (max over agents) from episode 5: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    max_t = 100\n",
    "    for t in range(max_t):\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Algorithm\n",
    "\n",
    "In this notebook, the agents were trained using the MADDPG algorithm as described in [this paper](https://arxiv.org/abs/1706.02275). Details of the implementation are as follows:\n",
    "\n",
    "* **Neural network architectures**  \n",
    "\n",
    "    The Actor(policy) network architecture is a multilayer perceptron (MLP) network with 2 hidden fully connected (fc) layers and 2 batch norm layers. The 1st fc layer had 128 units and the 2nd fc layer had 64 units. Relu was used as activation function after each fc layer. The batch norm layers were placed after each relu activation function. The final output layer used a tanh layer to bound the actions.\n",
    "\n",
    "    The Critic(value) network architecture is also a multilayer perceptron (MLP) network with 2 hidden fc layers and 1 batch norm layer. The batch norm layer was placed right after the input layer (state vectors from all agents) and before the 1st fc layer. The 1st fc layer had 128 units. Then, full actions vector (from all agents) were concatenated with the output of the 1st fc layer to feed into the 2nd fc layer. The 2nd fc layer had 64 units. Relu was used as activation function after each hidden layer. The final output layer has a single unit.\n",
    "\n",
    "    The final layer weights of both the actor and critic network were initialized from a uniform distribution [-3e-3, 3e-3]. This was to ensure the initial outputs for the policy and value estimates were near zero. The other layers were initialized from uniform distributions [-1/sqrt(f), 1/sqrt(f)], where f is the number of input units of the layer.  \n",
    "\n",
    "\n",
    "* **Hyperparameter**\n",
    "    * Experience replay buffer size = 1e6\n",
    "    * Minibatch size = 256\n",
    "    * Discount factor = 0.99\n",
    "    * Soft update parameter $\\tau$ = 5e-2              \n",
    "    * Actor learning rate = 1e-3         \n",
    "    * Critic learning rate = 1e-3         \n",
    "    * L2 weight decay rate = 0         \n",
    "    * Update the networks 4 times for every 4 timesteps\n",
    "    * Start learning only after at least 5000 experiences have been collected \n",
    "    \n",
    "    \n",
    "* **Important differences between MADDPG and DDPG**\n",
    "    - **DDPG agents use shared actor (policy) and critic function (value function)**  \n",
    "    In naive DDPG, all agents uses a single actor and critic function, i.e. all agents' actor and critic neural networks have the same weights. While each MADDPG agent has its own actor and critic neural networks and does not share the weight parameters.\n",
    "    \n",
    "    - **MADDPG uses state and action info from all agents**    \n",
    "    MADDPG's critic function uses state and action vectors from all agent as input but the actor function only uses individual agent's state vector. Naive DDPG agents do not consider interactions with other agents. \n",
    "    \n",
    "    - **MADDPG agent re-uses its own experience in memory storage**  \n",
    "    In naive DDPG, all experience samples are pooled together (as shared experiences) and used for traing without regard to which agent has generated it. While in MADDPG, during training, each agent only uses its own state vector as input to generate actions (although the critic function uses all agents' state and action vectors as input).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experiments and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Environment and Agent\n",
    "from maddpg import Agent\n",
    "\n",
    "agent = Agent(num_agents=num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a.Train the Agent with MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: 0.01\tTime lapsed: 0.1min\n",
      "Episode 400\tAverage Score: 0.04\tTime lapsed: 1.3min\n",
      "Episode 600\tAverage Score: 0.08\tTime lapsed: 4.4min\n",
      "Episode 800\tAverage Score: 0.11\tTime lapsed: 9.1min\n",
      "Episode 1000\tAverage Score: 0.21\tTime lapsed: 16.3min\n",
      "Episode 1200\tAverage Score: 0.25\tTime lapsed: 27.0min\n",
      "Episode 1400\tAverage Score: 0.38\tTime lapsed: 42.5min\n",
      "Episode 1600\tAverage Score: 0.42\tTime lapsed: 61.6min\n",
      "Episode 1800\tAverage Score: 0.56\tTime lapsed: 85.1min\n",
      "Episode 1875\tAverage Score: 0.71\tTime lapsed: 98.3min\n",
      "Environment solved in 1775 episodes!\tAverage Score: 0.71\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def maddpg_trainer(n_episodes=2000, max_t=1000, print_every=200):\n",
    "    \"\"\"MADDPG trainer\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        print_every (int): print scores every 'print_every' episodes\n",
    "    \"\"\"\n",
    "    final_scores = []                        # list containing final scores from each episode\n",
    "    final_scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    final_scores_window_avg = []             # avg of the final_scores_window\n",
    "    t1 = time()\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        agent.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations \n",
    "        scores = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations   # get the next states\n",
    "            rewards = env_info.rewards                   # get the rewards\n",
    "            dones = env_info.local_done \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if all(dones):\n",
    "                break \n",
    "        final_scores_window.append(np.max(scores))       # save most recent scores\n",
    "        final_scores.append(np.max(scores))              # save most recent scores\n",
    "        final_scores_window_avg.append(np.mean(final_scores_window)) # save most recent avg scores\n",
    "        t2 = (time()-t1)/60\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tTime lapsed: {:.1f}min'.format(i_episode, np.mean(final_scores_window), t2), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tTime lapsed: {:.1f}min'.format(i_episode, np.mean(final_scores_window), t2))\n",
    "        if final_scores_window_avg[-1] >= 0.7:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(final_scores_window)))\n",
    "            agent.save_model()\n",
    "            break\n",
    "    return final_scores, final_scores_window_avg\n",
    "\n",
    "\n",
    "final_scores, final_scores_avg = maddpg_trainer()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(final_scores)+1), final_scores)\n",
    "plt.plot(np.arange(1, len(final_scores)+1), final_scores_avg)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5b. Evaluate the trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\tScores: [1.40000002 1.29000002]\tMax score: 1.4000000208616257\n",
      "Episode: 1\tScores: [2.20000003 2.19000003]\tMax score: 2.2000000327825546\n",
      "Episode: 2\tScores: [1.90000003 1.79000003]\tMax score: 1.9000000283122063\n",
      "Episode: 3\tScores: [0.70000001 0.69000001]\tMax score: 0.7000000104308128\n",
      "Episode: 4\tScores: [0.40000001 0.39000001]\tMax score: 0.4000000059604645\n"
     ]
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent = Agent(num_agents=num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "agent.load_model()\n",
    "\n",
    "# play 5 episodes by the ddpg trained agent\n",
    "for i in range(5):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    states = env_info.vector_observations            # get the current state\n",
    "    scores = np.zeros(num_agents)                   # initialize the score\n",
    "    max_t = 1000\n",
    "    for t in range(max_t):\n",
    "        actions = agent.act(states)                      # select actions\n",
    "        env_info = env.step(actions)[brain_name]        # send the actions to the environment\n",
    "        next_states = env_info.vector_observations   # get the next states\n",
    "        rewards = env_info.rewards                   # get the rewards\n",
    "        dones = env_info.local_done                  # see if episodes have finished\n",
    "        scores += rewards                                # update the scores\n",
    "        states = next_states                             # roll over the states to next time step\n",
    "        if all(dones):                                       # exit loop if all episodes finished\n",
    "            break\n",
    "    print(\"Episode: {}\\tScores: {}\\tMax score: {}\".format(i, scores, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Watch the intelligent Agent play\n",
    "As can be seen in the movie clip below, the trained players can keep the tennis ball boucing back and forth without falling to the ground or out of bounds for many rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](tennis.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion  \n",
    "\n",
    "In this project, we solved the reacher environment in 1775 episodes. The training agents got average scores of above 0.7 in 100 consecutive episodes. To evaluate the trained model, we loaded new agents with network weights from the saved model. The reloaded agents were able to achieve average scores of ~1.32, well above the benchmark score of 0.5 in all of the first 5 episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Future work\n",
    "1. In this work, the hyperparameters for network initialization, learning rates, soft update parameter etc. were directly borrowed from the [DDPG paper](https://arxiv.org/abs/1509.02971). Fine-tuning those parameters specifically for this environment might boost the agents' performance further.\n",
    "2. Batch normalization technique was not used in the current implementation of the actor or critic networks. According to the [DDPG paper](https://arxiv.org/abs/1509.02971), batch normalization can minimize covariance shift and therefore stablize training. Adding batch normalization layers to the actor or critic networks may be beneficial to the networks' performance.\n",
    "3. DQN algorithm is known to tend to over-estimate state-action value function. Using [double Q-learning algorithm](https://arxiv.org/abs/1509.06461) as the estimator of state-action value function (the critic) may provide more accurate value function estimates and stablize learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
